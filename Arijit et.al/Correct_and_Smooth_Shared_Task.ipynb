{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck6qo0_JsiJD"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "import tensorflow as tf\n",
        "from keras import layers, models\n",
        "from keras.utils import to_categorical\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nKQUtDQzf2s",
        "outputId": "26f6febf-c3c5-4659-e95a-454971153094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed temp_file_cae70c31-93b7-469b-a997-e8a478de75f5.csv\n",
            "Removed temp_file_9d310ceb-710f-43f8-a0b8-a5371670d37d.csv\n",
            "Removed temp_file_c1bc8f24-4176-4c21-ba81-271e29e73191.csv\n",
            "Removed temp_file_5a0e633f-3403-4dbb-b21d-a2501e54b8fe.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def clear_temp_files():\n",
        "    # Use glob to find all files matching the pattern 'temp_file_*.csv'\n",
        "    temp_files = glob.glob('temp_file_*.csv')\n",
        "\n",
        "    # Iterate through each file in the list of temporary files\n",
        "    for file_path in temp_files:\n",
        "        try:\n",
        "            # Attempt to remove the file\n",
        "            os.remove(file_path)\n",
        "            # Print a message indicating that the file has been removed\n",
        "            print(f\"Removed {file_path}\")\n",
        "        except Exception as e:\n",
        "            # If an exception occurs during file removal, print an error message\n",
        "            print(f\"Error occurred while deleting file {file_path}: {e}\")\n",
        "\n",
        "# Call the function to clear temporary files\n",
        "clear_temp_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dyJOcG7s74n"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import uuid\n",
        "\n",
        "def get_download_url(file_path_or_url):\n",
        "    # Check if the provided path or URL is from Google Drive\n",
        "    if 'drive.google.com' in file_path_or_url:\n",
        "        # Extract the file ID from the Google Drive URL\n",
        "        file_id = file_path_or_url.split('/')[-2]\n",
        "        # Construct the download URL with the file ID\n",
        "        download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "        print(f\"Downloading from {download_url}\")\n",
        "\n",
        "        # Create a session to handle the download request\n",
        "        session = requests.Session()\n",
        "        response = session.get(download_url, stream=True)\n",
        "\n",
        "        # Check for the 'download_warning' cookie in the response\n",
        "        token = None\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                token = value\n",
        "                break\n",
        "\n",
        "        # If there's a confirmation token, make another request including it\n",
        "        if token:\n",
        "            params = {'id': file_id, 'confirm': token}\n",
        "            response = session.get(download_url, params=params, stream=True)\n",
        "\n",
        "        # Check the response headers to confirm successful file download\n",
        "        if 'content-disposition' in response.headers:\n",
        "            # Generate a unique file name for the downloaded file\n",
        "            unique_file_name = f'temp_file_{uuid.uuid4()}.csv'\n",
        "            # Write the downloaded content to the unique file\n",
        "            with open(unique_file_name, 'wb') as f:\n",
        "                for chunk in response.iter_content(32768):\n",
        "                    if chunk:  # Filter out keep-alive new chunks\n",
        "                        f.write(chunk)\n",
        "            # Update the file_path_or_url to the path of the downloaded file\n",
        "            file_path_or_url = unique_file_name\n",
        "        else:\n",
        "            print(\"Failed to download the file from Google Drive.\")\n",
        "            return None\n",
        "\n",
        "    # Return the updated file path or URL\n",
        "    return file_path_or_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLYRgI9w-GBr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_homophily(adjacency_matrix, Y):\n",
        "    # Print the shape of the attribute matrix Y (for debugging or information)\n",
        "    print(Y.shape)\n",
        "\n",
        "    # Initialize counters for same-attribute edges and total edges\n",
        "    same_attribute_count = 0\n",
        "    total_edge_count = np.sum(adjacency_matrix)\n",
        "\n",
        "    # Check for zero division to avoid division by zero\n",
        "    if total_edge_count == 0:\n",
        "        return 0\n",
        "\n",
        "    # Iterate over non-zero entries in the adjacency matrix\n",
        "    for i, j in np.argwhere(adjacency_matrix > 0):\n",
        "        # Check if nodes i and j have the same attribute\n",
        "        if str(Y[i]) == str(Y[j]):\n",
        "            # Increment the count of edges with the same attribute\n",
        "            same_attribute_count += adjacency_matrix[i][j]\n",
        "\n",
        "    # Calculate homophily as the ratio of same-attribute edges to total edges\n",
        "    homophily = same_attribute_count / total_edge_count\n",
        "    # Round the homophily percentage to two decimal places\n",
        "    return round(homophily * 100, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH2JTN92T0ti"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "def handle_datasets1(file_path_or_url, dataset):\n",
        "    # Read the dataset from the specified file path or URL\n",
        "    df = pd.read_csv(file_path_or_url, sep='\\t', header=None, low_memory=False, encoding='unicode_escape')\n",
        "\n",
        "    # Shuffle the rows of the DataFrame to randomize the order\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Extract identifiers (assumed to be in the first column)\n",
        "    identifiers = df.iloc[:, 0]\n",
        "\n",
        "    # Create an empty undirected graph using NetworkX\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph based on identifiers\n",
        "    for node in identifiers:\n",
        "        graph.add_node(str(node).strip())  # Assuming node identifiers are strings\n",
        "\n",
        "    # Extract features (X) from the DataFrame, handling NaN values by replacing with 0\n",
        "    X = df.iloc[:, 1:-1]\n",
        "    X = np.nan_to_num(X, nan=0)\n",
        "\n",
        "    # Extract labels (Y) from the last column of the DataFrame\n",
        "    Y = df.iloc[:, -1]\n",
        "\n",
        "    # Call the add_edges function to add edges to the graph based on the specified dataset\n",
        "    adjacency_matrix = add_edges(dataset, datasets[dataset]['edges'], graph)\n",
        "\n",
        "    # Return the extracted features, labels, identifiers, and adjacency matrix\n",
        "    return X, Y, identifiers, adjacency_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnHBpd20UNVT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "def handle_datasets2(file_path_or_url, dataset):\n",
        "    # Read the dataset from the specified file path or URL with handling of NaN values\n",
        "    df = pd.read_csv(file_path_or_url, low_memory=False)\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Print the column names of the DataFrame\n",
        "    print(df.columns)\n",
        "\n",
        "    # Shuffle the rows of the DataFrame to randomize the order\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Apply the string_to_float function to convert 'MedianIncome' to float\n",
        "    df['MedianIncome'] = df.apply(string_to_float, axis=1)\n",
        "\n",
        "    # Extract identifiers (assumed to be in the first column)\n",
        "    identifiers = df.iloc[:, 0]\n",
        "\n",
        "    # Create an empty undirected graph using NetworkX\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph based on identifiers\n",
        "    for node in identifiers:\n",
        "        graph.add_node(str(node).strip())  # Assuming node identifiers are strings\n",
        "\n",
        "    # Extract features (X) from the DataFrame, starting from the 5th column\n",
        "    X = df.iloc[:, 4:]\n",
        "    X = np.nan_to_num(X, nan=0)\n",
        "\n",
        "    # Apply the compare_and_assign function to assign labels to each row\n",
        "    df['label'] = df.apply(compare_and_assign, axis=1)\n",
        "\n",
        "    # Extract labels (Y) from the 'label' column\n",
        "    Y = df.loc[:, 'label']\n",
        "\n",
        "    # Print the number of NA values in the 'label' column\n",
        "    print('NA values: ', df['label'].isna().sum())\n",
        "\n",
        "    # Call the add_edges function to add edges to the graph based on the specified dataset\n",
        "    adjacency_matrix = add_edges(dataset, datasets[dataset]['edges'], graph)\n",
        "\n",
        "    # Return the extracted features, labels, identifiers, and adjacency matrix\n",
        "    return X, Y, identifiers, adjacency_matrix\n",
        "\n",
        "def compare_and_assign(row):\n",
        "    # Compare 'DEM' and 'GOP' values and assign label accordingly\n",
        "    if pd.notna(row['DEM']) and pd.notna(row['GOP']):\n",
        "        if row['DEM'] > row['GOP']:\n",
        "            return 'DEM'\n",
        "        elif row['GOP'] > row['DEM']:\n",
        "            return 'GOP'\n",
        "    return pd.NA\n",
        "\n",
        "def string_to_float(row):\n",
        "    # Convert 'MedianIncome' from string to float\n",
        "    return float(''.join(row['MedianIncome'].split(',')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQnGOXCRcx11"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "import pickle as pkl\n",
        "import random\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def handle_datasets3(dataset):\n",
        "    # Define the names of files corresponding to different data components\n",
        "    names = ['tx', 'ty', 'allx', 'ally', 'graph']\n",
        "\n",
        "    # Initialize a list to store loaded objects from files\n",
        "    objects = []\n",
        "\n",
        "    # Loop through each file name and load the corresponding object\n",
        "    for name in names:\n",
        "        # Get the file path or URL from the dataset dictionary\n",
        "        file_path_or_url = datasets[dataset][name]\n",
        "\n",
        "        # Call get_download_url to handle Google Drive links and download the file\n",
        "        file_path = get_download_url(file_path_or_url)\n",
        "\n",
        "        # Open the file and load the object using pickle\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # Use pickle to load the object (handles Python 2 and 3)\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    # Unpack the loaded objects into individual variables\n",
        "    tx, ty, allx, ally, graph = tuple(objects)\n",
        "\n",
        "    # Stack feature matrices vertically and convert to List of Lists (lil) format\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "\n",
        "    # Stack label matrices vertically\n",
        "    labels = np.vstack((ally, ty))\n",
        "\n",
        "    # Create a NetworkX graph from the graph representation\n",
        "    graph = nx.from_dict_of_lists(graph)\n",
        "\n",
        "    # Shuffling\n",
        "    nodes = list(graph.nodes())\n",
        "    indices = list(range(len(nodes)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # Apply shuffling to features and labels using the shuffled indices\n",
        "    features = features[indices, :]\n",
        "    labels = labels[indices, :]\n",
        "\n",
        "    # Relabel nodes in the graph based on shuffled indices\n",
        "    shuffled_graph = nx.relabel_nodes(graph, {old: nodes[new] for new, old in enumerate(indices)})\n",
        "\n",
        "    # Convert the shuffled graph to an adjacency matrix\n",
        "    adj = nx.adjacency_matrix(shuffled_graph)\n",
        "\n",
        "    # Return the processed features, labels, list of shuffled nodes, and adjacency matrix\n",
        "    return features, pd.DataFrame(labels).idxmax(axis=1), list(shuffled_graph.nodes()), adj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBVZB_u8i4OL"
      },
      "outputs": [],
      "source": [
        "def plot_edges(adj_matrix):\n",
        "    # Ensure the matrix is a numpy array\n",
        "    adj_matrix = np.array(adj_matrix)\n",
        "\n",
        "    # Extract the edge weights (excluding diagonal elements)\n",
        "    edge_weights = adj_matrix[np.triu_indices(adj_matrix.shape[0], 1)]\n",
        "\n",
        "    # Determine the number of bins (rule of thumb: square root of the number of data points)\n",
        "    num_bins = int(np.sqrt(len(edge_weights)))\n",
        "\n",
        "    # Plotting the distribution of edge weights with a logarithmic scale\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Use a log scale on y-axis if the bin count is zero\n",
        "    plt.hist(edge_weights, bins=num_bins, color='blue', edgecolor='black', log=True)\n",
        "    plt.title('Distribution of Edge Weights in Adjacency Matrix (Log Scale)')\n",
        "    plt.xlabel('Edge Weight')\n",
        "    plt.ylabel('Frequency (Log Scale)')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPowqoMX-vhF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_adjacency_matrix(X, threshold=0.5):\n",
        "    # Convert the input features to a NumPy array\n",
        "    X = np.array(X)\n",
        "\n",
        "    # Compute the cosine similarity matrix between feature vectors\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "    # Create an adjacency matrix by thresholding the similarity matrix\n",
        "    adjacency_matrix = np.where(similarity_matrix > threshold, similarity_matrix, 0)\n",
        "\n",
        "    return adjacency_matrix\n",
        "\n",
        "def read_dataset(file_path_function, dataset_key):\n",
        "    # Get the file path using the provided function and dataset key\n",
        "    file_path = file_path_function(dataset_key)\n",
        "\n",
        "    # Read the dataset from the specified file path\n",
        "    return pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "def visualise_better(adjacency_matrix):\n",
        "    # Create a graph from the adjacency matrix\n",
        "    G = nx.Graph(adjacency_matrix)\n",
        "\n",
        "    # Layout the graph using the Kamada-Kawai layout algorithm\n",
        "    pos = nx.kamada_kawai_layout(G)\n",
        "\n",
        "    # Compute node degrees for adjusting node sizes\n",
        "    degrees = np.array([val for (node, val) in G.degree()])\n",
        "    node_size = (degrees / np.max(degrees)) * 1000\n",
        "\n",
        "    # Visualize the graph with improved layout and adjusted node sizes\n",
        "    nx.draw(G, pos, node_size=node_size, node_color='skyblue', font_size=8,\n",
        "            font_color='black', font_weight='bold', with_labels=True, alpha=0.7)\n",
        "\n",
        "    # Set plot title and display the graph\n",
        "    plt.title(\"Improved Graph Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "def concatenate_datasets(dataset, keys):\n",
        "    # Read datasets for the specified keys and concatenate them\n",
        "    dataframes = [read_dataset(get_download_url, dataset[key]) for key in keys]\n",
        "    return pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "def handle_datasets4(file_path_or_url, datasetname):\n",
        "    # Concatenate 'train' and 'dev' datasets along with their embeddings\n",
        "    data = concatenate_datasets(datasets[datasetname], ['train', 'dev'])\n",
        "    embeddings = concatenate_datasets(datasets[datasetname], ['trainembedding', 'devembedding'])\n",
        "\n",
        "    # Extract labels and embeddings from the concatenated datasets\n",
        "    Y = data['label']\n",
        "    embeddings['embeddings'] = embeddings['embeddings'].apply(lambda x: np.array(eval(x)))\n",
        "    X = np.vstack(embeddings['embeddings'].values)\n",
        "\n",
        "    # Extract identifiers and create the adjacency matrix\n",
        "    identifiers = data['id']\n",
        "    adj = create_adjacency_matrix(X)\n",
        "\n",
        "    # Visualize the graph edges for better understanding\n",
        "    visualise_better(adj)\n",
        "\n",
        "    # Return the processed features, labels, identifiers, and adjacency matrix\n",
        "    return X, Y, identifiers, adj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m204TCwuS1u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import traceback  # Import traceback module for handling exceptions\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    try:\n",
        "        # Determine the file path or URL based on the dataset\n",
        "        if dataset == 'pubmed' or 'sharedtask' in dataset:\n",
        "            file_path_or_url = dataset\n",
        "        else:\n",
        "            file_path_or_url = get_download_url(datasets[dataset]['features'])\n",
        "            print(\"File path: \", datasets[dataset]['features'])\n",
        "\n",
        "        # Get the handler function for processing the dataset\n",
        "        handler = datasets[dataset]['handler']\n",
        "\n",
        "        # Call the handler function to load and process the dataset\n",
        "        X, Y, identifiers, adjacency_matrix = handler(file_path_or_url, dataset)\n",
        "\n",
        "        # Display information about the loaded dataset\n",
        "        print(\"Number of nodes (X rows): \", X.shape[0])\n",
        "        print(\"Number of features (X columns): \", X.shape[1])\n",
        "        print(\"Number of classes or outputs (Y columns): \", Y.nunique())\n",
        "        print(\"Number of edges in graph: \", adjacency_matrix.sum())\n",
        "        print(f\"Number of nodes in graph {adjacency_matrix.shape[0]}\")\n",
        "\n",
        "        # Calculate and display the homophily of the graph\n",
        "        homophily_percentage = calculate_homophily(adjacency_matrix, Y)\n",
        "        print(f\"Homophily of graph {homophily_percentage}%\")\n",
        "\n",
        "        # Return the processed data\n",
        "        return X, Y, identifiers, adjacency_matrix\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Handle the case where the file is not found\n",
        "        print(f\"The file {dataset} was not found.\")\n",
        "    except Exception as e:\n",
        "        # Handle other exceptions and print the traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruXiA0HLr4Mg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def add_edges(dataset, edges_file_path, G):\n",
        "    # Print a message indicating the start of loading the adjacency matrix\n",
        "    print(\"Loading adjacency_matrix\")\n",
        "\n",
        "    # Get the file path or URL for the edges file\n",
        "    file_path_or_url = get_download_url(edges_file_path)\n",
        "\n",
        "    # Raise an exception if the file download fails\n",
        "    if file_path_or_url is None:\n",
        "        raise FileNotFoundError(f\"Failed to download the dataset: {edges_file_path}\")\n",
        "\n",
        "    # Set to store erroneous nodes (nodes mentioned in the edges file but not present in the graph)\n",
        "    erroneous_nodes = set()\n",
        "\n",
        "    # Function to check if a node is present in the graph\n",
        "    def check_node(name, G):\n",
        "        if not G.has_node(name):\n",
        "            erroneous_nodes.add(name)\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    # Open the edges file and process each line\n",
        "    with open(file_path_or_url, 'r') as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            # Split the line into nodes based on the dataset type\n",
        "            if dataset == 'cora' or dataset == 'citeseer':\n",
        "                nodes = line.strip().split()\n",
        "            elif dataset == 'us-county':\n",
        "                # Skip the header line (first line) for the 'us-county' dataset\n",
        "                if idx == 0:\n",
        "                    continue\n",
        "                nodes = line.strip().split(',')\n",
        "\n",
        "            # Check if there are two nodes in the line\n",
        "            if len(nodes) == 2:\n",
        "                node1 = str(nodes[0])\n",
        "                node2 = str(nodes[1])\n",
        "\n",
        "                # Add an edge between the nodes if they exist in the graph\n",
        "                if check_node(node1, G) and check_node(node2, G):\n",
        "                    G.add_edge(node1, node2)\n",
        "\n",
        "    # Print the number of erroneous nodes found\n",
        "    print(len(erroneous_nodes), \"erroneous nodes found..\")\n",
        "\n",
        "    # Get the adjacency matrix from the graph\n",
        "    adjacency_matrix = nx.adjacency_matrix(G)\n",
        "\n",
        "    # Return the adjacency matrix\n",
        "    return adjacency_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkORZU63lWBc"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import SpectralEmbedding\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def spectral_embedding(X, A):\n",
        "    # Set the number of components for spectral embedding\n",
        "    n_components = 5\n",
        "\n",
        "    # Calculate the normalized Laplacian matrix\n",
        "    D = np.diag(np.sum(A, axis=1))\n",
        "    L = D - A\n",
        "\n",
        "    # Calculate the spectral embedding using nearest neighbors affinity\n",
        "    se = SpectralEmbedding(n_components=n_components, random_state=42, affinity='nearest_neighbors')\n",
        "    embedding = se.fit_transform(X)\n",
        "\n",
        "    # Standardize the spectral embedding features\n",
        "    scaler = StandardScaler()\n",
        "    embedding = scaler.fit_transform(embedding)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "def train_linear_se_model(X_train, y_train, A):\n",
        "    # Perform spectral embedding on the graph and concatenate with original features\n",
        "    embedding = spectral_embedding(X_train, A)\n",
        "    X_train_se = np.concatenate((X_train, embedding), axis=1)\n",
        "\n",
        "    # Normalize the features\n",
        "    X_train_se = normalize_features(X_train_se)\n",
        "\n",
        "    # Train a linear model (Logistic Regression) on the enhanced features\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_se, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_mlp_se_model(X_train, y_train, A, validation_fraction):\n",
        "    # Perform spectral embedding on the graph and concatenate with original features\n",
        "    embedding = spectral_embedding(X_train, A)\n",
        "    X_train_se = np.concatenate((X_train, embedding), axis=1)\n",
        "\n",
        "    # Normalize the features\n",
        "    X_train_se = normalize_features(X_train_se)\n",
        "\n",
        "    # Train an MLP model with hyperparameter tuning and early stopping on the enhanced features\n",
        "    parameters = {'hidden_layer_sizes': [(50,), (100,)],\n",
        "                  'alpha': [0.001, 0.01],\n",
        "                  'learning_rate_init': [0.001, 0.01]}\n",
        "\n",
        "    mlp = MLPClassifier(max_iter=500, early_stopping=True, validation_fraction=validation_fraction)\n",
        "    grid_search = GridSearchCV(mlp, parameters, cv=2, scoring='neg_log_loss', n_jobs=-1)\n",
        "    grid_search.fit(X_train_se, y_train)\n",
        "\n",
        "    # Select the best model from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96AmKVe1wFLm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_linear_model(X_train, y_train):\n",
        "    # Train a linear model (Logistic Regression)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_mlp_model(X_train, y_train, validation_fraction):\n",
        "    # Train an MLP model with hyperparameter tuning and early stopping\n",
        "    parameters = {'hidden_layer_sizes': [(50,), (100,)],\n",
        "                  'alpha': [0.001, 0.01],\n",
        "                  'learning_rate_init': [0.001, 0.01]}\n",
        "\n",
        "    mlp = MLPClassifier(max_iter=500, early_stopping=True, validation_fraction=validation_fraction)\n",
        "    # Note: validation_fraction specifies the proportion of training data to set aside as the validation set for early stopping\n",
        "\n",
        "    grid_search = GridSearchCV(mlp, parameters, cv=2, scoring='neg_log_loss', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    # Select the best model from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def train_base_predictor(X_train, y_train, X, model_type='linear', adj=np.zeros((1, 1)), validation_fraction=0.1):\n",
        "    if model_type == 'linear':\n",
        "        # Train a linear model and return the predicted probabilities\n",
        "        base_model = train_linear_model(X_train, y_train)\n",
        "    elif model_type == 'mlp':\n",
        "        # Train an MLP model and return the predicted probabilities\n",
        "        base_model = train_mlp_model(X_train, y_train, validation_fraction)\n",
        "    elif model_type == 'linear_se':\n",
        "        # Train a linear model with spectral embedding and return the predicted probabilities\n",
        "        base_model = train_linear_se_model(X_train, y_train, adj)\n",
        "    elif model_type == 'mlp_se':\n",
        "        # Train an MLP model with spectral embedding and return the predicted probabilities\n",
        "        base_model = train_mlp_se_model(X_train, y_train, adj, validation_fraction)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_type. Choose 'linear', 'mlp', 'linear_se', or 'mlp_se'.\")\n",
        "\n",
        "    # If the model is linear or MLP, predict probabilities using the original or enhanced features\n",
        "    if model_type == 'linear' or model_type == 'mlp':\n",
        "        return base_model.predict_proba(X)\n",
        "    else:\n",
        "        # For spectral embedding models, concatenate with spectral embedding, normalize, and predict probabilities\n",
        "        X_expand = np.concatenate((X, spectral_embedding(X, adj)), axis=1)\n",
        "        X_expand = normalize_features(X_expand)\n",
        "        return base_model.predict_proba(X_expand)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GxFKEffVdtH"
      },
      "outputs": [],
      "source": [
        "# ERROR PROPAGATION\n",
        "\n",
        "def calculate_normalized_adjacency_matrix(A):\n",
        "    # Calculate the normalized adjacency matrix S from the original adjacency matrix A\n",
        "    D = np.sum(A, axis=1)\n",
        "    D_sqrt_inv = np.diag(1. / np.sqrt(D))\n",
        "    D_sqrt_inv[np.isinf(D_sqrt_inv)] = 0\n",
        "    S = D_sqrt_inv @ A @ D_sqrt_inv\n",
        "    return S\n",
        "\n",
        "def calculate_residuals(Y, Z):\n",
        "    # Calculate the residuals (difference) between true labels Y and predicted labels Z\n",
        "    E = Y - Z\n",
        "    return E\n",
        "\n",
        "def label_spreading(E, S, alpha, max_iter=50):\n",
        "    # Perform label spreading to correct errors in predictions\n",
        "    W = np.copy(E)\n",
        "    for _ in range(max_iter):\n",
        "        W_new = (1 - alpha) * W + alpha * np.dot(S, W)\n",
        "        diff = np.linalg.norm(W - W_new)\n",
        "        if diff < 0.0001:\n",
        "            break\n",
        "        W = W_new\n",
        "    return W\n",
        "\n",
        "def correct_predictions(Z, E_hat):\n",
        "    # Correct predictions by adding the error estimates (E_hat) to the original predictions (Z)\n",
        "    Z_corrected = Z + E_hat\n",
        "    return Z_corrected\n",
        "\n",
        "def error_propagation(Z, Y, A, train_end, validation_end, alpha_values=[0.001, 0.01, 0.02, 0.05, 0.1], max_iter=100):\n",
        "    # Error propagation algorithm to refine predictions and find the best alpha value\n",
        "\n",
        "    # Calculate the normalized adjacency matrix S\n",
        "    S = calculate_normalized_adjacency_matrix(A)\n",
        "\n",
        "    # Calculate residuals (errors) between true labels Y and initial predictions Z\n",
        "    E = calculate_residuals(Y, Z)\n",
        "\n",
        "    # Set errors in the validation and test sets to zero\n",
        "    E_with_val = np.copy(E)\n",
        "    E_with_val[validation_end:, :] = 0\n",
        "    E[train_end:, :] = 0\n",
        "\n",
        "    # Best alpha initialization\n",
        "    best_alpha = None\n",
        "    max_acc = 0\n",
        "    output_values = None\n",
        "\n",
        "    # Iterate over possible alpha values to find the best one\n",
        "    for alpha in alpha_values:\n",
        "        F = np.copy(E)\n",
        "        F_hat = label_spreading(F, S, alpha, max_iter)\n",
        "        Z_corrected = correct_predictions(np.copy(Z), F_hat)\n",
        "\n",
        "        # Compute accuracy on the validation set\n",
        "        validation_acc = accuracy_score(np.argmax(Y[train_end:validation_end], axis=1),\n",
        "                                         np.argmax(Z_corrected[train_end:validation_end], axis=1))\n",
        "        print(f\"alpha {alpha}, validation_acc {validation_acc}\")\n",
        "\n",
        "        # Update best alpha if needed\n",
        "        if validation_acc >= max_acc:\n",
        "            max_acc = validation_acc\n",
        "            best_alpha = alpha\n",
        "            output_values = (Z_corrected, F_hat, F)\n",
        "\n",
        "    # Re-run error propagation using the best alpha on the entire validation set\n",
        "    F = np.copy(E_with_val)\n",
        "    F_hat = label_spreading(F, S, best_alpha, max_iter)\n",
        "    Z_corrected = correct_predictions(np.copy(Z), F_hat)\n",
        "    output_values = (Z_corrected, F_hat, F)\n",
        "\n",
        "    print(\"Best alpha:\", best_alpha)\n",
        "    return output_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6nLcUHuBml-"
      },
      "outputs": [],
      "source": [
        "# SCALING\n",
        "\n",
        "def autoscale(E_hat, E, L):\n",
        "    # Autoscale the error estimates E_hat based on the residuals E\n",
        "    E_auto = np.copy(E_hat)\n",
        "\n",
        "    # Reshape E_auto if it's 1-dimensional\n",
        "    if E_hat.ndim == 1:\n",
        "        E_auto = E_auto.reshape(-1, 1)\n",
        "\n",
        "    # Compute sigma as the mean L1 norm of residuals for training instances\n",
        "    sigma = np.mean(np.linalg.norm(E[:L], ord=1, axis=1))\n",
        "    sigma = max(sigma, 1e-10)  # Avoid division by zero\n",
        "\n",
        "    # Compute the L1 norm of the error estimates for validation and test instances\n",
        "    norm_E_hat = np.linalg.norm(E_auto[L:], ord=1, axis=1, keepdims=True)\n",
        "    norm_E_hat = np.maximum(norm_E_hat, 1e-10)  # Avoid division by zero\n",
        "\n",
        "    # Scale the error estimates based on the computed sigma\n",
        "    E_auto[L:] *= (sigma / norm_E_hat)\n",
        "\n",
        "    return E_auto\n",
        "\n",
        "\n",
        "def scaled_fixed_diffusion(E, A, val_end, max_iter=100, error_margin=1e-6):\n",
        "    # Scaled Fixed Diffusion to adjust the scale of the residual\n",
        "    D = np.diag(np.sum(A, axis=0))\n",
        "    D_inv = np.linalg.inv(D)\n",
        "    E_fixed = np.copy(E)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        E_fixed[:val_end] = E[:val_end]\n",
        "        tmp = A @ E_fixed\n",
        "        res = D_inv @ tmp\n",
        "        E_fixed[val_end:] = res[val_end:]\n",
        "\n",
        "    return E_fixed\n",
        "\n",
        "def apply_scaling_methods(Z_corrected, E_hat, E, scaling_type, A, val_end):\n",
        "    # Apply scaling methods and correct the predictions\n",
        "\n",
        "    if scaling_type == 'autoscale':\n",
        "        scaled_autoscale = autoscale(E_hat, E, val_end)\n",
        "        return Z_corrected + scaled_autoscale\n",
        "    elif scaling_type == 'fixed_diffusion':\n",
        "        scaled_fixed_diff = scaled_fixed_diffusion(E_hat, A, val_end)\n",
        "        return Z_corrected + scaled_fixed_diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P449qa3BqMT"
      },
      "outputs": [],
      "source": [
        "# SMOOTHING FINAL PREDICTIONS\n",
        "\n",
        "def final_label_propagation(Z_corrected, Y, A, train_end, validation_end, alpha_values=[0.001, 0.01, 0.02, 0.05, 0.1], max_iter=100):\n",
        "    # Function to perform final label propagation to smooth the predictions\n",
        "\n",
        "    def propagate_labels(H, S, alpha, max_iter):\n",
        "        # Propagate labels using a label propagation algorithm\n",
        "        for _ in range(max_iter):\n",
        "            H_new = (1 - alpha) * H + alpha * np.dot(S, H)\n",
        "            diff = np.linalg.norm(H - H_new)\n",
        "            if diff < 0.0001:\n",
        "                break\n",
        "            H = H_new\n",
        "        return H\n",
        "\n",
        "    # Calculate the normalized adjacency matrix\n",
        "    S = calculate_normalized_adjacency_matrix(A)\n",
        "\n",
        "    # Initialize the result matrix as a copy of corrected predictions\n",
        "    H = np.copy(Z_corrected)\n",
        "\n",
        "    # Perform label propagation with a specific alpha value (e.g., 0.01)\n",
        "    H = propagate_labels(H, S, 0.01, max_iter)\n",
        "\n",
        "    # Keep the training labels intact (up to the validation_end index)\n",
        "    H[:validation_end] = Y[:validation_end]\n",
        "\n",
        "    return H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rvoCm4Hg-yO"
      },
      "outputs": [],
      "source": [
        "# EVALUATE ACCURACIES\n",
        "\n",
        "def evaluate_accuracies(Y, Z, name, val_end):\n",
        "    \"\"\"\n",
        "    Evaluate accuracy and print the result.\n",
        "\n",
        "    Parameters:\n",
        "    - Y: True labels\n",
        "    - Z: Predicted labels\n",
        "    - name: Name or identifier for the evaluation method\n",
        "    - val_end: Index indicating the end of the validation set\n",
        "\n",
        "    Returns:\n",
        "    - acc: Accuracy as a percentage\n",
        "    \"\"\"\n",
        "    # Calculate accuracy using true and predicted labels\n",
        "    acc = accuracy_score(Y[val_end:], np.argmax(Z[val_end:], axis=1))\n",
        "\n",
        "    # Round accuracy to two decimal places\n",
        "    acc = round(acc * 100, 2)\n",
        "\n",
        "    # Print the accuracy result\n",
        "    print(f\"Accuracy after {name}: {acc}%\")\n",
        "\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GDPeAykCC4b"
      },
      "outputs": [],
      "source": [
        "# NORMALIZE FEATURES\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "def normalize_features(X):\n",
        "    \"\"\"\n",
        "    Normalize the features of a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Input feature matrix\n",
        "\n",
        "    Returns:\n",
        "    - X_normalized: Normalized feature matrix\n",
        "    \"\"\"\n",
        "    # Create a MinMaxScaler object\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit and transform the input features using MinMaxScaler\n",
        "    X_normalized = min_max_scaler.fit_transform(X)\n",
        "\n",
        "    # Return the normalized feature matrix\n",
        "    return X_normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nluM54C-zCjV",
        "outputId": "6d617823-7383-4ca8-ad48-30e41437213a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Training on sharedtaskbert dataset ---\n",
            "Loading dataset...\n",
            "Downloading from https://drive.google.com/uc?export=download&id=15XtL2MReWk1s_Hw5FzUdhEUX8OByxKaQ\n",
            "Downloading from https://drive.google.com/uc?export=download&id=1j3shDaPEq8RUeDUz1X0pc6I0jGABpGyF\n",
            "Downloading from https://drive.google.com/uc?export=download&id=1RzdRA2zYjeM9bZNCRUNRNBCIwR5zd7wv\n",
            "Downloading from https://drive.google.com/uc?export=download&id=1xElM3C6JiRso0yn94XLGROA9qtl6Ayqp\n"
          ]
        }
      ],
      "source": [
        "# Define a dictionary containing information about different datasets\n",
        "datasets = {\n",
        "    # 'citeseer': {\n",
        "    #     'features':\"https://drive.google.com/file/d/1Jxb4mR8sT92Rc2tjBZf2jTo9X9g19iNh/view?usp=drive_link\",\n",
        "    #     'edges':\"https://drive.google.com/file/d/15VeHRUjnCuGCqkz3fieLAivNl35C9bG-/view?usp=drive_link\",\n",
        "    #     'train_split': 0.6,\n",
        "    #     'test_split': 0.2,\n",
        "    #     'validation_split': 0.2,\n",
        "    #     'handler':handle_datasets1\n",
        "    # },\n",
        "    # 'us-county': {\n",
        "    #     'features':\"https://drive.google.com/file/d/1OKa4otThbZWZq2cscmN-4MC0zYaG2hXF/view?usp=drive_link\",\n",
        "    #     'edges':\"https://drive.google.com/file/d/1MRS3XcqAYeV53EFAP-FPhghxkU8uaPSR/view?usp=drive_link\",\n",
        "    #     'train_split': 0.4,\n",
        "    #     'test_split': 0.5,\n",
        "    #     'validation_split': 0.1,\n",
        "    #     'handler':handle_datasets2\n",
        "\n",
        "    # },\n",
        "    #     'cora': {\n",
        "    #     'features':\"https://drive.google.com/file/d/1vypw02B3Rq6Knen9JBu0kZ-PPbCEQsbJ/view?usp=drive_link\",\n",
        "    #     'edges':\"https://drive.google.com/file/d/16S7tezMQK6ksqtCxhQwyBkP99i5IdYKE/view?usp=drive_link\",\n",
        "        # 'train_split': 0.6,\n",
        "        # 'test_split': 0.2,\n",
        "        # 'validation_split': 0.2,\n",
        "    #     'handler':handle_datasets2\n",
        "    # },\n",
        "    # 'pubmed': {\n",
        "    #     'tx':\"https://drive.google.com/file/d/1eNbbzB5h0kmqNt1Lk6sTMeX1V3RO52DE/view?usp=drive_link\",\n",
        "    #     'ty':\"https://drive.google.com/file/d/1Sfr4TzMrO4iFUGTmoXumNsccVHSsoAua/view?usp=drive_link\",\n",
        "    #     'graph':\"https://drive.google.com/file/d/1T-ImSb-X9KgiIvcA-Aj9hYh8AWoh103S/view?usp=drive_link\",\n",
        "    #     'allx':\"https://drive.google.com/file/d/1tg2qFpzJrNkkSC3OkaktHs4joshwi2Lq/view?usp=drive_link\",\n",
        "    #     'ally':\"https://drive.google.com/file/d/1sX27bffBGa7TKh_Uoc6qHAyex7biHxoP/view?usp=drive_link\",\n",
        "\n",
        "    #     'train_split': 0.6,\n",
        "    #     'test_split': 0.2,\n",
        "    #     'validation_split': 0.2,\n",
        "    #     'handler':handle_datasets3\n",
        "    # },\n",
        "\n",
        "    # 'sharedtasksgcn': {\n",
        "    #     'dev':\"https://drive.google.com/file/d/1j3shDaPEq8RUeDUz1X0pc6I0jGABpGyF/view?usp=drive_link\",\n",
        "    #     'train':\"https://drive.google.com/file/d/15XtL2MReWk1s_Hw5FzUdhEUX8OByxKaQ/view?usp=drive_link\",\n",
        "    #     'devembedding':\"https://drive.google.com/file/d/180Krz-rID0XeTCyIeKeleY4AHwexo8n2/view?usp=drive_link\",\n",
        "    #     'trainembedding':\"https://drive.google.com/file/d/1SVth-3S3PewWJjFd_Qny_Mm2UJNtAzFT/view?usp=drive_link\",\n",
        "    #     'handler':handle_datasets4,\n",
        "    #     'train_split':0.85,\n",
        "    #     'test_split': 0.075,\n",
        "    #     'validation_split': 0.075,\n",
        "    # },\n",
        "\n",
        "    'sharedtaskbert': {\n",
        "        'dev':\"https://drive.google.com/file/d/1j3shDaPEq8RUeDUz1X0pc6I0jGABpGyF/view?usp=drive_link\",\n",
        "        'train':\"https://drive.google.com/file/d/15XtL2MReWk1s_Hw5FzUdhEUX8OByxKaQ/view?usp=drive_link\",\n",
        "        'devembedding':\"https://drive.google.com/file/d/1xElM3C6JiRso0yn94XLGROA9qtl6Ayqp/view?usp=drive_link\",\n",
        "        'trainembedding':\"https://drive.google.com/file/d/1RzdRA2zYjeM9bZNCRUNRNBCIwR5zd7wv/view?usp=drive_link\",\n",
        "        'handler':handle_datasets4,\n",
        "        'train_split':0.85,\n",
        "        'test_split': 0.075,\n",
        "        'validation_split': 0.075,\n",
        "\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Dictionary to store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Iterate through each dataset in the dictionary\n",
        "for dataset_name, dataset_info in datasets.items():\n",
        "    print(f\"\\n\\n--- Training on {dataset_name} dataset ---\")\n",
        "\n",
        "    # Load the dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    X, Y, identifiers,adjacency_matrix = load_dataset(dataset_name)\n",
        "    X=normalize_features(X)\n",
        "\n",
        "    # Preprocessing labels\n",
        "    print(\"Preprocessing labels...\")\n",
        "    label_encoder = LabelEncoder()\n",
        "    Y_numerical = label_encoder.fit_transform(Y)\n",
        "    Y_onehot = to_categorical(Y_numerical)\n",
        "\n",
        "    # Calculate the split indices\n",
        "    train_end=  int(dataset_info['train_split'] * X.shape[0])\n",
        "    validation_end = int((dataset_info['train_split'] +dataset_info['validation_split']) * X.shape[0])\n",
        "\n",
        "    # Plot label distributions\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    Y[:train_end].value_counts().plot(kind='bar', title=dataset_name + ' Training label distribution', ax=axs[0, 0])\n",
        "    Y[train_end:validation_end].value_counts().plot(kind='bar', title=dataset_name + ' Validation label distribution', ax=axs[0, 1])\n",
        "    Y[validation_end:].value_counts().plot(kind='bar', title=dataset_name + ' Testing label distribution', ax=axs[1, 0])\n",
        "    Y.value_counts().plot(kind='bar', title=dataset_name + ' label distribution', ax=axs[1, 1])\n",
        "\n",
        "    fig.text(0.5, -0.1, f'Number of Nodes: {X.shape[0]}', ha='center', va='top')\n",
        "    fig.text(0.5, -0.15, f'Homophily: {calculate_homophily(adjacency_matrix, Y)}%', ha='center', va='top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Iterate through each model type\n",
        "    for model_type in ['linear', 'mlp', 'linear_se', 'mlp_se']:\n",
        "        accuracies[(dataset_name, model_type)] = {}\n",
        "\n",
        "        # Split the datasets\n",
        "        X_train = X[:train_end]\n",
        "        Y_train = Y_numerical[:train_end]\n",
        "\n",
        "        X_train_val = X[:validation_end]\n",
        "        Y_train_val = Y_numerical[:validation_end]\n",
        "\n",
        "        X_test = X[validation_end:]\n",
        "        Y_test_onehot = Y_onehot[validation_end:]\n",
        "\n",
        "\n",
        "        # Train base predictor and evaluate\n",
        "        print(f\"Training {model_type.upper()} base predictor...\")\n",
        "        start_time = time.time()\n",
        "        Z = train_base_predictor(X_train_val if ('mlp' in model_type) else X_train, Y_train_val if ('mlp' in model_type) else Y_train, X, model_type, adj=adjacency_matrix if ('se' in model_type) else None, validation_fraction=dataset_info['validation_split'] / dataset_info['train_split'] if (model_type == 'mlp' or model_type == 'mlp_se') else None)\n",
        "        end_time = time.time()\n",
        "        print(f\"Time taken for {model_type.upper()} base predictor training: {end_time - start_time} seconds\")\n",
        "        accuracies[(dataset_name, model_type)]['base'] = evaluate_accuracies(Y_numerical, Z, f\"{model_type.upper()} base\", validation_end)\n",
        "\n",
        "        # Error propagation\n",
        "        print(f\"Performing error propagation for {model_type}...\")\n",
        "        start_time = time.time()\n",
        "        Z_corrected, E_hat, E = error_propagation(Z, Y_onehot, adjacency_matrix, train_end,validation_end)\n",
        "        end_time = time.time()\n",
        "        print(f\"Time taken for error propagation for {model_type}: {end_time - start_time} seconds\")\n",
        "        accuracies[(dataset_name, model_type)]['error_propagation'] = evaluate_accuracies(Y_numerical, Z_corrected, f\"{model_type} error propagation\", validation_end)\n",
        "\n",
        "        # Scaling\n",
        "        scaling_methods=['autoscale','fixed_diffusion']\n",
        "        for scaling_method in scaling_methods:\n",
        "          name=model_type+\" \"+ scaling_method\n",
        "          print(f\"Applying scaling method : {name}...\")\n",
        "          start_time = time.time()\n",
        "          Z_scaled = apply_scaling_methods(Z_corrected, E_hat, E,scaling_method, adjacency_matrix, validation_end)\n",
        "          end_time = time.time()\n",
        "          print(f\"Time taken for scaling method for {name} : {end_time - start_time} seconds\")\n",
        "          accuracies[(dataset_name, model_type)][scaling_method] = evaluate_accuracies(Y_numerical, Z_scaled, name, validation_end)\n",
        "\n",
        "          # Smoothing of final predictions\n",
        "          print(f\"Performing final label propagation smoothing for {name}...\")\n",
        "          start_time = time.time()\n",
        "          Ypred = final_label_propagation(Z_scaled, Y_onehot, adjacency_matrix, train_end,validation_end,max_iter= 100)\n",
        "          end_time = time.time()\n",
        "          print(f\"Time taken for final label propagation smoothing for {name} {end_time - start_time} seconds\")\n",
        "          accuracies[(dataset_name, model_type)][\"Label Propagation \"+scaling_method] = evaluate_accuracies(Y_numerical, Ypred, \"Label Propagation \"+name, validation_end)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEu1LW7vW-n2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to highlight maximum values in specified rows of a DataFrame\n",
        "def highlight_max_in_rows(df, color='lightgreen'):\n",
        "    attr = f'background-color: {color}; color: black'\n",
        "\n",
        "    # Initialize a DataFrame with empty strings\n",
        "    result = pd.DataFrame('', index=df.index, columns=df.columns)\n",
        "\n",
        "    # Identify rows to highlight (usually the last two rows if more than 5 rows)\n",
        "    row_indices = [3, 5] if len(df) >= 6 else df.index[-2:]\n",
        "\n",
        "    # Iterate through columns in groups of 4\n",
        "    for i in range(0, len(df.columns), 4):\n",
        "        group_columns = df.columns[i:i+4]\n",
        "\n",
        "        # Find the maximum value in the specified rows and columns\n",
        "        max_value = df[group_columns].iloc[row_indices].max().max()\n",
        "\n",
        "        # Highlight the maximum values in the result DataFrame\n",
        "        for row_idx in row_indices:\n",
        "            row = df.index[row_idx]\n",
        "            result.loc[row, group_columns] = [attr if df.at[row, col] == max_value else '' for col in group_columns]\n",
        "    return result\n",
        "\n",
        "\n",
        "# Function to display accuracies DataFrame with styling\n",
        "def display_accuracies(accuracies):\n",
        "\n",
        "    # Convert the accuracies dictionary to a DataFrame\n",
        "    df = pd.DataFrame(accuracies)\n",
        "\n",
        "    # Apply basic styling (black text on white background)\n",
        "    styled_df = df.style.apply(lambda x: ['color: black'] * len(x), axis=1)\\\n",
        "                         .apply(lambda x: ['background-color: white'] * len(x), axis=1)\\\n",
        "                         .apply(highlight_max_in_rows, axis=None)\n",
        "\n",
        "    # Define border styles for every 4th column to separate groups\n",
        "    border_styles = []\n",
        "    for i in range(5, len(df.columns) + 1, 4):\n",
        "        border_styles.append({'selector': f'th:nth-child({i}), td:nth-child({i})',\n",
        "                              'props': [('border-right', '2px solid')]})\n",
        "\n",
        "    # Apply additional styling for highlighted cells\n",
        "    styled_df.set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', 'black'), ('color', 'white')]},\n",
        "        {'selector': 'td', 'props': [('font-weight', 'bold')]}\n",
        "    ] + border_styles, overwrite=False)\n",
        "\n",
        "    return styled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3qDRlS2p6vl"
      },
      "outputs": [],
      "source": [
        "display_accuracies(accuracies)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
